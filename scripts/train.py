#!/usr/bin/env python3
"""
è¿‡æ‹Ÿåˆä¿®å¤è®­ç»ƒè„šæœ¬ - è§£å†³ä¸¥é‡è¿‡æ‹Ÿåˆé—®é¢˜

ç­–ç•¥ï¼š
1. ç®€åŒ–æ¨¡å‹æ¶æ„
2. å¢å¼ºæ­£åˆ™åŒ–
3. ç‰¹å¾é€‰æ‹©
4. ä¼˜åŒ–è®­ç»ƒç­–ç•¥
"""

import pandas as pd
import numpy as np
import os
import sys
import pickle
import tensorflow as tf
from tensorflow import keras
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import logging
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('..')

# å¯¼å…¥æ¨¡å—
from configs import sampling_config as config

def simplified_feature_engineering(train_df, test_df, output_dir):
    """ç®€åŒ–çš„ç‰¹å¾å·¥ç¨‹ - å‡å°‘å¤æ‚åº¦"""
    logger.info("å¼€å§‹ç®€åŒ–ç‰¹å¾å·¥ç¨‹...")
    
    # 1. åŸºç¡€ä»·æ ¼ç‰¹å¾å¤„ç†
    if 'price' in train_df.columns:
        # ä½¿ç”¨RobustScalerå¤„ç†å¼‚å¸¸å€¼
        robust_scaler = RobustScaler()
        train_df['price_robust'] = robust_scaler.fit_transform(train_df[['price']])
        test_df['price_robust'] = robust_scaler.transform(test_df[['price']])
        
        # ä¿å­˜scalers
        with open(os.path.join(output_dir, 'simplified_robust_scaler.pkl'), 'wb') as f:
            pickle.dump(robust_scaler, f)
    
    # 2. ç®€åŒ–çš„äº¤äº’ç‰¹å¾ - åªä¿ç•™æœ€é‡è¦çš„
    if all(feat in train_df.columns for feat in ['age_level', 'final_gender_code']):
        train_df['age_gender'] = train_df['age_level'].astype(str) + '_' + train_df['final_gender_code'].astype(str)
        test_df['age_gender'] = test_df['age_level'].astype(str) + '_' + test_df['final_gender_code'].astype(str)
    
    # 3. ç®€åŒ–çš„ç»Ÿè®¡ç‰¹å¾
    if 'user_id' in train_df.columns:
        # åªä¿ç•™æœ€é‡è¦çš„ç”¨æˆ·ç»Ÿè®¡
        user_stats = train_df.groupby('user_id').agg({
            'label': ['count', 'mean'],
            'price': ['mean']
        }).fillna(0)
        user_stats.columns = ['user_click_count', 'user_click_rate', 'user_avg_price']
        
        train_df = train_df.merge(user_stats, on='user_id', how='left')
        test_df = test_df.merge(user_stats, on='user_id', how='left')
        
        # å¡«å……ç¼ºå¤±å€¼
        for col in user_stats.columns:
            train_df[col] = train_df[col].fillna(0)
            test_df[col] = test_df[col].fillna(0)
    
    # 4. é‡æ–°ç¼–ç æ ¸å¿ƒåˆ†ç±»ç‰¹å¾
    categorical_features = [
        'cms_segid', 'final_gender_code', 'age_level', 'pvalue_level',
        'cate_id', 'brand', 'age_gender'
    ]
    
    feature_encoders = {}
    
    for feat in categorical_features:
        if feat in train_df.columns:
            le = LabelEncoder()
            # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ‰€æœ‰å”¯ä¸€å€¼
            all_values = pd.concat([train_df[feat], test_df[feat]]).unique()
            le.fit(all_values)
            
            train_df[feat] = le.transform(train_df[feat])
            test_df[feat] = le.transform(test_df[feat])
            
            feature_encoders[feat] = le
    
    # 5. ç®€åŒ–çš„ç‰¹å¾é€‰æ‹©
    numeric_features = ['price_robust', 'user_click_count', 'user_click_rate', 'user_avg_price']
    
    if len(numeric_features) > 0:
        # é€‰æ‹©æœ€é‡è¦çš„æ•°å€¼ç‰¹å¾
        selector = SelectKBest(score_func=f_classif, k=min(3, len(numeric_features)))
        X_train_numeric = train_df[numeric_features].fillna(0)
        X_test_numeric = test_df[numeric_features].fillna(0)
        
        X_train_selected = selector.fit_transform(X_train_numeric, train_df['label'])
        X_test_selected = selector.transform(X_test_numeric)
        
        selected_features = [numeric_features[i] for i in selector.get_support(indices=True)]
        
        # å°†é€‰æ‹©çš„ç‰¹å¾æ·»åŠ åˆ°æ•°æ®æ¡†
        for i, feat in enumerate(selected_features):
            train_df[f'{feat}_selected'] = X_train_selected[:, i]
            test_df[f'{feat}_selected'] = X_test_selected[:, i]
    
    # ä¿å­˜ç¼–ç å™¨
    with open(os.path.join(output_dir, 'simplified_feature_encoders.pkl'), 'wb') as f:
        pickle.dump(feature_encoders, f)
    
    logger.info("ç®€åŒ–ç‰¹å¾å·¥ç¨‹å®Œæˆ")
    return train_df, test_df, feature_encoders

def create_simplified_model_config():
    """åˆ›å»ºç®€åŒ–çš„æ¨¡å‹é…ç½®"""
    return {
        'EMBEDDING_DIM': 32,  # é™ä½embeddingç»´åº¦
        'DNN_UNITS': [128, 64],  # å‡å°‘åˆ°2å±‚
        'TEMP': 0.1,  # é€‚ä¸­çš„æ¸©åº¦å‚æ•°
        'LEARNING_RATE': 0.0001,  # æ›´å°çš„å­¦ä¹ ç‡
        'BATCH_SIZE': 1024,  # æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
        'EPOCHS': 30,  # å‡å°‘è®­ç»ƒè½®æ•°
        'DROPOUT_RATE': 0.5,  # æ›´å¼ºçš„dropout
        'L2_REG': 1e-3,  # æ›´å¼ºçš„L2æ­£åˆ™åŒ–
        'BATCH_NORM': True,  # ä¿ç•™æ‰¹å½’ä¸€åŒ–
        'RESIDUAL': False  # ç§»é™¤æ®‹å·®è¿æ¥
    }

def simplified_dssm_model(user_feature_columns, item_feature_columns, config_dict):
    """ç®€åŒ–çš„DSSMæ¨¡å‹"""
    import tensorflow as tf
    from collections import namedtuple
    
    # ä½¿ç”¨ä¸src.modelç›¸åŒçš„namedtupleå®šä¹‰
    SparseFeat = namedtuple('SparseFeat', ['name', 'vocabulary_size', 'embedding_dim'])
    DenseFeat = namedtuple('DenseFeat', ['name', 'dimension'])
    
    # åˆ›å»ºè¾“å…¥å±‚
    input_layers = {}
    logger.info(f"åˆ›å»ºç®€åŒ–è¾“å…¥å±‚ï¼Œç‰¹å¾åˆ—æ•°é‡: {len(user_feature_columns + item_feature_columns)}")
    
    for feat in user_feature_columns + item_feature_columns:
        logger.info(f"å¤„ç†ç‰¹å¾: {feat.name}, ç±»å‹: {type(feat)}")
        if hasattr(feat, 'vocabulary_size'):  # SparseFeat
            input_layers[feat.name] = tf.keras.layers.Input(shape=(1,), name=feat.name)
            logger.info(f"åˆ›å»ºSparseFeatè¾“å…¥å±‚: {feat.name}")
        elif hasattr(feat, 'dimension'):  # DenseFeat
            input_layers[feat.name] = tf.keras.layers.Input(shape=(feat.dimension,), name=feat.name)
            logger.info(f"åˆ›å»ºDenseFeatè¾“å…¥å±‚: {feat.name}")
        else:
            logger.warning(f"æœªçŸ¥ç‰¹å¾ç±»å‹: {type(feat)} for {feat}")
    
    logger.info(f"åˆ›å»ºçš„è¾“å…¥å±‚: {list(input_layers.keys())}")
    
    # ç”¨æˆ·å¡”
    user_embeddings = []
    for feat in user_feature_columns:
        if hasattr(feat, 'vocabulary_size'):  # SparseFeat
            embedding = tf.keras.layers.Embedding(
                feat.vocabulary_size, 
                feat.embedding_dim, 
                name=f'user_emb_{feat.name}'
            )(input_layers[feat.name])
            user_embeddings.append(embedding)
    
    if user_embeddings:
        user_concat = tf.keras.layers.Concatenate(axis=-1)(user_embeddings)
        user_flat = tf.keras.layers.Flatten()(user_concat)
    else:
        user_flat = tf.keras.layers.Dense(32, activation='relu')(tf.keras.layers.Input(shape=(1,)))
    
    # ç”¨æˆ·DNN - ç®€åŒ–ç‰ˆæœ¬
    user_dnn = user_flat
    for i, units in enumerate(config_dict['DNN_UNITS']):
        if config_dict['BATCH_NORM']:
            user_dnn = tf.keras.layers.BatchNormalization()(user_dnn)
        
        user_dnn = tf.keras.layers.Dense(
            units, 
            activation='relu',
            kernel_regularizer=tf.keras.regularizers.l2(config_dict['L2_REG']),
            name=f'user_dnn_{i}'
        )(user_dnn)
        
        user_dnn = tf.keras.layers.Dropout(config_dict['DROPOUT_RATE'])(user_dnn)
    
    # ç‰©å“å¡”
    item_embeddings = []
    item_dense = []
    
    for feat in item_feature_columns:
        if hasattr(feat, 'vocabulary_size'):  # SparseFeat
            embedding = tf.keras.layers.Embedding(
                feat.vocabulary_size, 
                feat.embedding_dim, 
                name=f'item_emb_{feat.name}'
            )(input_layers[feat.name])
            item_embeddings.append(embedding)
        elif hasattr(feat, 'dimension'):  # DenseFeat
            item_dense.append(input_layers[feat.name])
    
    if item_embeddings:
        item_concat = tf.keras.layers.Concatenate(axis=-1)(item_embeddings)
        item_flat = tf.keras.layers.Flatten()(item_concat)
    else:
        item_flat = tf.keras.layers.Dense(32, activation='relu')(tf.keras.layers.Input(shape=(1,)))
    
    if item_dense:
        item_dense_concat = tf.keras.layers.Concatenate(axis=-1)(item_dense)
        item_all = tf.keras.layers.Concatenate(axis=-1)([item_flat, item_dense_concat])
    else:
        item_all = item_flat
    
    # ç‰©å“DNN - ç®€åŒ–ç‰ˆæœ¬
    item_dnn = item_all
    for i, units in enumerate(config_dict['DNN_UNITS']):
        if config_dict['BATCH_NORM']:
            item_dnn = tf.keras.layers.BatchNormalization()(item_dnn)
        
        item_dnn = tf.keras.layers.Dense(
            units, 
            activation='relu',
            kernel_regularizer=tf.keras.regularizers.l2(config_dict['L2_REG']),
            name=f'item_dnn_{i}'
        )(item_dnn)
        
        item_dnn = tf.keras.layers.Dropout(config_dict['DROPOUT_RATE'])(item_dnn)
    
    # è®¡ç®—ç›¸ä¼¼åº¦ - ä½¿ç”¨Keraså±‚
    user_norm = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(user_dnn)
    item_norm = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(item_dnn)
    
    similarity = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True))([user_norm, item_norm])
    similarity = tf.keras.layers.Lambda(lambda x: x * config_dict['TEMP'])(similarity)
    
    # è¾“å‡ºå±‚
    output = tf.keras.layers.Dense(1, activation='sigmoid')(similarity)
    
    # æ„å»ºæ¨¡å‹
    model = tf.keras.Model(inputs=list(input_layers.values()), outputs=output)
    
    return model

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # å®šä¹‰ç‰¹å¾ç±»å‹
    from collections import namedtuple
    SparseFeat = namedtuple('SparseFeat', ['name', 'vocabulary_size', 'embedding_dim'])
    DenseFeat = namedtuple('DenseFeat', ['name', 'dimension'])
    
    # ==================== è·¯å¾„é…ç½® ====================
    # æœ¬åœ°è®­ç»ƒè·¯å¾„é…ç½®
    LOCAL_DATA_PATH = '/Users/masicheng/Desktop/æœå¹¿æ¨/taobao-dssm-project/data'
    LOCAL_OUTPUT_DIR = '/Users/masicheng/Desktop/æœå¹¿æ¨/taobao-dssm-project/outputs'
    
    # äº‘ç«¯è®­ç»ƒè·¯å¾„é…ç½®
    CLOUD_DATA_PATH = './data'
    CLOUD_OUTPUT_DIR = './outputs'
    
    # ==================== è·¯å¾„åˆ‡æ¢è¯´æ˜ ====================
    # æœ¬åœ°å¿«é€Ÿè®­ç»ƒï¼ˆä½¿ç”¨ç°æœ‰é‡‡æ ·æ•°æ®ï¼‰ï¼š
    # DATA_PATH = LOCAL_DATA_PATH
    # OUTPUT_DIR = LOCAL_OUTPUT_DIR
    # æ³¨æ„ï¼šéœ€è¦å…ˆè¿è¡Œ local_sampling.py ç”Ÿæˆé‡‡æ ·æ•°æ®
    
    # äº‘ç«¯å®Œæ•´è®­ç»ƒï¼ˆå¤„ç†å…¨éƒ¨æ•°æ®ï¼‰ï¼š
    # DATA_PATH = CLOUD_DATA_PATH  
    # OUTPUT_DIR = CLOUD_OUTPUT_DIR
    # æ³¨æ„ï¼šéœ€è¦å…ˆè¿è¡Œ process_data.py å¤„ç†å®Œæ•´æ•°æ®
    
    # å½“å‰ä½¿ç”¨çš„è·¯å¾„é…ç½®ï¼ˆæ‰‹åŠ¨åˆ‡æ¢ï¼‰
    DATA_PATH = LOCAL_DATA_PATH  # åˆ‡æ¢ä¸º CLOUD_DATA_PATH ç”¨äºäº‘ç«¯
    OUTPUT_DIR = LOCAL_OUTPUT_DIR  # åˆ‡æ¢ä¸º CLOUD_OUTPUT_DIR ç”¨äºäº‘ç«¯
    
    logger.info(f"=== å½“å‰è·¯å¾„é…ç½® ===")
    logger.info(f"æ•°æ®è·¯å¾„: {DATA_PATH}")
    logger.info(f"è¾“å‡ºè·¯å¾„: {OUTPUT_DIR}")
    logger.info("=== è¿‡æ‹Ÿåˆä¿®å¤çš„Taobao DSSMè®­ç»ƒå¼€å§‹ ===")
    
    logger.info("=== è¿‡æ‹Ÿåˆä¿®å¤çš„Taobao DSSMè®­ç»ƒå¼€å§‹ ===")
    
    # æ­¥éª¤1: åŠ è½½æ”¹è¿›çš„æ•°æ®
    logger.info("\næ­¥éª¤1: åŠ è½½æ”¹è¿›çš„æ•°æ®")
    train_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_data_improved.csv'))
    test_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'test_data_improved.csv'))
    
    logger.info(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_df.shape}")
    logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}")
    
    # æ­¥éª¤2: ç®€åŒ–ç‰¹å¾å·¥ç¨‹
    logger.info("\næ­¥éª¤2: ç®€åŒ–ç‰¹å¾å·¥ç¨‹")
    train_df, test_df, feature_encoders = simplified_feature_engineering(
        train_df, test_df, OUTPUT_DIR
    )
    
    logger.info(f"ç®€åŒ–ç‰¹å¾å·¥ç¨‹åè®­ç»ƒæ•°æ®å½¢çŠ¶: {train_df.shape}")
    logger.info(f"ç®€åŒ–ç‰¹å¾å·¥ç¨‹åæµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}")
    
    # æ­¥éª¤3: åˆ›å»ºç®€åŒ–æ¨¡å‹é…ç½®
    simplified_config = create_simplified_model_config()
    logger.info(f"ç®€åŒ–æ¨¡å‹é…ç½®: {simplified_config}")
    
    # æ­¥éª¤4: å‡†å¤‡æ¨¡å‹è¾“å…¥
    logger.info("\næ­¥éª¤4: å‡†å¤‡æ¨¡å‹è¾“å…¥")
    
    # å®šä¹‰ç®€åŒ–çš„ç‰¹å¾åˆ—è¡¨
    user_features = [
        'cms_segid', 'final_gender_code', 'age_level', 'pvalue_level', 'age_gender'
    ]
    
    item_features = [
        'cate_id', 'brand', 'price_robust'
    ]
    
    # æ·»åŠ æ•°å€¼ç‰¹å¾
    numeric_features = [col for col in train_df.columns if col.endswith('_selected')]
    item_features.extend(numeric_features)
    
    # è¿‡æ»¤å‡ºå®é™…å­˜åœ¨çš„ç‰¹å¾
    available_features = train_df.columns.tolist()
    user_features = [feat for feat in user_features if feat in available_features]
    item_features = [feat for feat in item_features if feat in available_features]
    
    logger.info(f"ç®€åŒ–ç”¨æˆ·ç‰¹å¾: {user_features}")
    logger.info(f"ç®€åŒ–ç‰©å“ç‰¹å¾: {item_features}")
    
    # åˆ›å»ºç‰¹å¾åˆ—
    user_feature_columns = []
    for feat in user_features:
        if feat in train_df.columns:
            vocab_size = len(feature_encoders[feat].classes_) + 1
            user_feature_columns.append(
                SparseFeat(
                    name=feat, 
                    vocabulary_size=vocab_size, 
                    embedding_dim=simplified_config['EMBEDDING_DIM']
                )
            )
    
    item_feature_columns = []
    for feat in item_features:
        if feat in train_df.columns:
            if feat in numeric_features or feat in ['price_robust']:
                item_feature_columns.append(
                    DenseFeat(name=feat, dimension=1)
                )
            else:
                vocab_size = len(feature_encoders[feat].classes_) + 1
                item_feature_columns.append(
                    SparseFeat(
                        name=feat, 
                        vocabulary_size=vocab_size, 
                        embedding_dim=simplified_config['EMBEDDING_DIM']
                    )
                )
    
    logger.info(f"ç”¨æˆ·ç‰¹å¾åˆ—æ•°: {len(user_feature_columns)}")
    logger.info(f"ç‰©å“ç‰¹å¾åˆ—æ•°: {len(item_feature_columns)}")
    
    # æ­¥éª¤5: æ„å»ºç®€åŒ–æ¨¡å‹
    logger.info("\næ­¥éª¤5: æ„å»ºç®€åŒ–æ¨¡å‹")
    dssm_model = simplified_dssm_model(
        user_feature_columns, 
        item_feature_columns, 
        simplified_config
    )
    
    dssm_model.summary()
    
    # ç¼–è¯‘æ¨¡å‹
    dssm_model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=simplified_config['LEARNING_RATE']),
        loss="binary_crossentropy", 
        metrics=[
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    logger.info("ç®€åŒ–æ¨¡å‹ç¼–è¯‘æˆåŠŸï¼")
    
    # æ­¥éª¤6: å‡†å¤‡è®­ç»ƒæ•°æ®
    logger.info("\næ­¥éª¤6: å‡†å¤‡è®­ç»ƒæ•°æ®")
    
    # åˆ›å»ºæ¨¡å‹è¾“å…¥å­—å…¸
    all_features = user_features + item_features
    train_model_input = {name: train_df[name] for name in all_features if name in train_df.columns}
    train_label = train_df['label'].values
    test_model_input = {name: test_df[name] for name in all_features if name in test_df.columns}
    test_label = test_df['label'].values
    
    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_label)}")
    logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_label)}")
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    pos_weight = len(train_df[train_df['label']==0]) / len(train_df[train_df['label']==1])
    class_weight = {0: 1, 1: pos_weight}
    logger.info(f"ç±»åˆ«æƒé‡: {class_weight}")
    
    # æ­¥éª¤7: è®­ç»ƒæ¨¡å‹
    logger.info("\næ­¥éª¤7: è®­ç»ƒç®€åŒ–æ¨¡å‹")
    
    # å›è°ƒå‡½æ•° - æ›´æ¿€è¿›çš„æ—©åœ
    early_stopping = EarlyStopping(
        monitor='val_auc', 
        mode='max', 
        patience=5,  # å‡å°‘patience
        verbose=1, 
        restore_best_weights=True
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,  # æ›´æ¸©å’Œçš„å­¦ä¹ ç‡è¡°å‡
        patience=3,
        min_lr=1e-6,
        verbose=1
    )
    
    model_checkpoint = ModelCheckpoint(
        filepath=os.path.join(OUTPUT_DIR, 'best_simplified_model.keras'),
        monitor='val_auc',
        mode='max',
        save_best_only=True,
        verbose=1
    )
    
    logger.info("å¼€å§‹ç®€åŒ–æ¨¡å‹è®­ç»ƒ...")
    
    history = dssm_model.fit(
        train_model_input, 
        train_label, 
        batch_size=simplified_config['BATCH_SIZE'],
        epochs=simplified_config['EPOCHS'],
        validation_data=(test_model_input, test_label),
        callbacks=[early_stopping, reduce_lr, model_checkpoint],
        class_weight=class_weight
    )
    
    logger.info("ç®€åŒ–æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    # æ­¥éª¤8: æ¨¡å‹è¯„ä¼°
    logger.info("\næ­¥éª¤8: æ¨¡å‹è¯„ä¼°")
    test_loss, test_auc, test_precision, test_recall = dssm_model.evaluate(test_model_input, test_label)
    logger.info(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    logger.info(f"æµ‹è¯•AUC: {test_auc:.4f}")
    logger.info(f"æµ‹è¯•ç²¾ç¡®ç‡: {test_precision:.4f}")
    logger.info(f"æµ‹è¯•å¬å›ç‡: {test_recall:.4f}")
    
    # è®¡ç®—F1åˆ†æ•°
    f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)
    logger.info(f"æµ‹è¯•F1åˆ†æ•°: {f1_score:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    model_save_path = os.path.join(OUTPUT_DIR, 'simplified_dssm_model.keras')
    dssm_model.save(model_save_path)
    logger.info(f"ç®€åŒ–æ¨¡å‹ä¿å­˜åˆ°: {model_save_path}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_save_path = os.path.join(OUTPUT_DIR, 'simplified_training_history.pkl')
    with open(history_save_path, 'wb') as f:
        pickle.dump(history.history, f)
    logger.info(f"ç®€åŒ–è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_save_path}")
    
    # è¾“å‡ºæ€§èƒ½æ€»ç»“
    print(f"\n=== è¿‡æ‹Ÿåˆä¿®å¤è®­ç»ƒæ€§èƒ½æ€»ç»“ ===")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_label):,}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_label):,}")
    print(f"æœ€ç»ˆæµ‹è¯•AUC: {test_auc:.4f}")
    print(f"æœ€ç»ˆæµ‹è¯•ç²¾ç¡®ç‡: {test_precision:.4f}")
    print(f"æœ€ç»ˆæµ‹è¯•å¬å›ç‡: {test_recall:.4f}")
    print(f"æœ€ç»ˆæµ‹è¯•F1åˆ†æ•°: {f1_score:.4f}")
    print(f"è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
    
    # åˆ†æè¿‡æ‹Ÿåˆç¨‹åº¦
    if len(history.history['auc']) > 0:
        final_train_auc = history.history['auc'][-1]
        overfitting_degree = final_train_auc - test_auc
        print(f"æœ€ç»ˆè®­ç»ƒAUC: {final_train_auc:.4f}")
        print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting_degree:.4f}")
        
        # åˆ¤æ–­æ”¹è¿›æ•ˆæœ
        if test_auc > 0.55:
            print(f"\nğŸ‰ æˆåŠŸï¼éªŒè¯AUCæå‡åˆ°{test_auc:.4f}ï¼Œæ˜¾è‘—æ”¹è¿›ï¼")
        elif test_auc > 0.54:
            print(f"\nâœ… æœ‰æ”¹å–„ï¼éªŒè¯AUCæå‡åˆ°{test_auc:.4f}ï¼Œå°å¹…æ”¹è¿›ã€‚")
        else:
            print(f"\nâš ï¸  éªŒè¯AUCä¸º{test_auc:.4f}ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜ã€‚")
        
        if overfitting_degree < 0.1:
            print(f"âœ… è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½ï¼è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting_degree:.4f}")
        elif overfitting_degree < 0.2:
            print(f"âš ï¸  è¿‡æ‹Ÿåˆç¨‹åº¦é€‚ä¸­: {overfitting_degree:.4f}")
        else:
            print(f"âŒ è¿‡æ‹Ÿåˆä»ç„¶ä¸¥é‡: {overfitting_degree:.4f}")
    
    logger.info("=== è¿‡æ‹Ÿåˆä¿®å¤è®­ç»ƒå®Œæˆ ===")
    
    return {
        'test_auc': test_auc,
        'test_loss': test_loss,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': f1_score,
        'train_samples': len(train_label),
        'test_samples': len(test_label),
        'model_path': model_save_path
    }

if __name__ == "__main__":
    try:
        results = main()
        print(f"\nè¿‡æ‹Ÿåˆä¿®å¤è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"æœ€ç»ˆéªŒè¯AUC: {results['test_auc']:.4f}")
        print(f"æœ€ç»ˆç²¾ç¡®ç‡: {results['test_precision']:.4f}")
        print(f"æœ€ç»ˆå¬å›ç‡: {results['test_recall']:.4f}")
        print(f"æœ€ç»ˆF1åˆ†æ•°: {results['test_f1']:.4f}")
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
